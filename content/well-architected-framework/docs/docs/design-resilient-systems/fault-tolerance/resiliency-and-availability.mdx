---
page_title: Resiliency and availability
description: Use HashiCorp tools in the design of your fault-tolerant systems to avoid disruptions from a single point of failure, and ensure the business continuity of your mission-critical deployments.
---

# Resiliency and availability

As you plan for resiliency and availability, you must decide how robust your system architecture needs to be in terms of failure, degradation, and performance. The following are key considerations that you should include when planning for availability and resilience in your deployment:

1. Identify all applications and infrastructure where availability is critical. 
1. Calculate the cost of your failure domain strategy.
1. Decide your uptime goals.
1. Compare your architecture and failure recovery plans to the business requirements (BCP).

HashiCorp resources:

- [Run a reliable Nomad cluster](/well-architected-framework/ir-nomad/implementation-resources-nomad-reliability)
- [Run a reliable Vault cluster](/well-architected-framework/ir-vault/implementation-resources-vault-reliability)

External resources:

- [Fault tolerance and fault isolation](https://docs.aws.amazon.com/whitepapers/latest/availability-and-beyond-improving-resilience/fault-tolerance-and-fault-isolation.html)
- [Designing resilient systems](https://cloud.google.com/compute/docs/tutorials/robustsystems)
- [Getting Started with Reliability on Azure: Ensuring Cloud Applications Stay Up and Running](https://techcommunity.microsoft.com/blog/azurearchitectureblog/getting-started-with-reliability-on-azure-ensuring-cloud-applications-stay-up-an/4152905)
- [Thinking like an architect: Understanding failure domains](https://www.ibm.com/blog/thinking-like-an-architect-understanding-failure-domains/)
- [What Could Possibly Go Wrong?](https://deploy.equinix.com/blog/explaining-failure-domains-sre-lifeblood/)
- [Uptime versus Availability: How to measure and improve reliability](https://www.pluralsight.com/resources/blog/tech-operations/uptime-availability-metrics-app-reliability)
- [Business continuity versus disaster recovery: Which plan is right for you?](https://www.ibm.com/think/topics/business-continuity-vs-disaster-recovery-plan)
- [Business Continuity Plan (BCP)](https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/business-continuity-plan-bcp.html)

## Consul

Consul has a range of features that operate both locally and remotely that can help you offer a resilient service across datacenters. Each Consul datacenter depends on a set of Consul voting server agents. The voting servers ensure Consul has a consistent, fault-tolerant state by requiring a majority of voting servers, known as a quorum, to agree upon state changes.

Consider the following factors when you use Consul to design your resilient architectures:

- **Cluster quorum**: Consul uses the Raft protocol to achieve consensus with a quorum (or majority) of operational servers, and can tolerate failure in one or more servers depending on quorum size. A Consul cluster will enter a read-only state to prevent data inconsistency if it loses quorum. You can also use Redundancy Zones in your Consul deployments to run one voter and any number of non-voters in each defined zone.
- **Data distribution and replication**: Consul replicates all data across cluster servers. Any server in the cluster can respond to read requests, but writes require consensus from a quorum of servers. Consul also automatically synchronizes data across the cluster when a failed server recovers.
- **Cluster leader election**: When the cluster leader fails, Consul automatically conducts an election to elect a new leader. The leader election requires a quorum of voter participation by cluster servers, and quorum constraints prevent split-brain scenarios.
- **Service discovery and resilience**: Consul clients keep local caches of service information, and can continue basic service discovery even when temporarily disconnected. Health checks also continue to function during partial cluster outages, and the anti-entropy system automatically repairs inconsistencies between the client's local state and cluster state.
- **Network partition handling**: A Consul cluster uses a gossip protocol to detect network partitions, and uses quorum requirements to enforce data consistency during network partitions. Servers in the minority partition enter read-only mode to prevent split-brain scenarios, and when the partition heals, the cluster automatically resynchronizes data.
- **Multi-datacenter support**: Each Consul datacenter independently operates with its own consensus group, and cross-datacenter replication continues even if some datacenters are unreachable. Consul also uses a WAN gossip pool to keep datacenter connectivity details.
- **Backup and recovery**: You can recover from catastrophic failures with data snapshots.

HashiCorp resources:

- [Fault tolerance](/consul/docs/concept/reliability)
- [Raft protocol](/consul/docs/concept/consensus)
- [Redundancy zones](/consul/tutorials/operate-consul/redundancy-zones)
- [Anti-Entropy Enforcement](/consul/docs/concept/consistency)
- [Backup Consul data and state](/consul/tutorials/operate-consul/backup-and-restore)

## Nomad

Nomad is a simple and flexible scheduler and orchestrator that deploys and manages containers and non-containerized applications across on-premises and clouds at scale. Your Nomad deployments can achieve fault tolerance, and offer resilience and availability to your use cases through the following key properties:

- **Cluster quorum**: Nomad uses the Raft protocol to achieve consensus with a quorum (or majority) of operational servers, and can tolerate failure in one or more servers depending on quorum size. If Nomad loses quorum, it enters a read-only state to prevent data inconsistency.
- **Node failure handling**: Nomad relies on a heartbeat mechanism to automatically detect node failure. Failed nodes which do not heartbeat get marked as down and the Nomad maintains the desired state by automatically rescheduling new instances of failed jobs on to operational nodes.
- **Job scheduling resilience**: Job scheduling supports automatic restarts for failed tasks with configurable restart attempts. Jobs can specify affinities and constraints to effectively spread across failure domains. Nomad's system jobs ensure instances run on all eligible nodes to support coverage, and rolling updates enable zero-downtime deployments with automatic rollbacks.
- **Data persistence**: Nomad's implementation of the Raft protocol ensures consistent state across cluster nodes by replicating server state across the cluster. Snapshot functionality enables back up and restoration of cluster state, while client nodes keep local state for running allocations.
- **Automatic scaling**: The Nomad Autoscaler is a separate tool that enables horizontal application and cluster scaling for Nomad clusters. Horizontal application autoscaling is the process of automatically controlling the number of instances of an application to gain work throughput to meet service-level agreements (SLA). Horizontal cluster autoscaling is the process of adding or removing Nomad clients from a cluster to ensure there is an appropriate amount of cluster resource for the scheduled applications.

HashiCorp resources:

- [Nomad reference architecture](/nomad/tutorials/enterprise/production-reference-architecture-vm-with-consul)
- [Consensus protocol](/nomad/docs/concepts/consensus)
- [Client Heartbeats](/nomad/docs/configuration/server#client-heartbeats)
- [Scheduling](/nomad/docs/concepts/scheduling)
- [Affinities and constraints](/nomad/docs/concepts/scheduling/placement#affinities-and-constraints)
- [Nomad Autoscaler overview](/nomad/tools/autoscaling)
- [Increase failure tolerance with spread](/nomad/tutorials/advanced-scheduling/spread)

## Vault

Vault is an identity-based secret and encryption management system that secures, stores, and controls access to tokens, passwords, certificates, and encryption keys for protecting secrets and other sensitive data. 

Vault clusters have some important resiliency and availability properties you should consider when you are designing fault-tolerant systems:

- **High availability architecture**: A cluster of Vault servers can run in high availability (HA) mode with an active server and standby servers. When operating in HA mode, Vault can automatically failover a non-operational active server, and use leader election to choose a new active server.
- **Automated Integrated Storage management**: Vault can automate the cluster management for Integrated Storage with the Autopilot feature to check server health, stabilize quorum, and periodically clean up failed servers.
- **Performance standbys**: By default, Vault standby servers forward all requests they receive to the active server. Vault Enterprise offers extra functionality that allow HA servers to service read requests which do not mutate storage directly from a standby node. These performance standby servers can distribute the load, enabling your cluster to scale horizontally and reduce latency for read-heavy use cases.
- **Replication**: Scaling your Vault cluster to meet performance demands is critical to ensure workloads operate efficiently. Vault Enterprise and HCP Vault Dedicated support multi-region deployments so you can replicate data to regional Vault clusters to support local workloads.

HashiCorp resources:

- [Vault with integrated storage reference architecture](/vault/tutorials/raft/raft-reference-architecture)
- [Configure Vault cluster with Integrated Storage](/vault/tutorials/raft/raft-storage)
- [Automate Integrated Storage management](/vault/tutorials/raft/raft-autopilot)
- [Autopilot](/vault/docs/concepts/integrated-storage/autopilot)
- [Automate upgrades with Vault Enterprise](/vault/tutorials/raft/raft-upgrade-automation)
- [Performance standby nodes](/vault/docs/enterprise/performance-standby)
- [Scale horizontally with performance standby nodes](/vault/tutorials/enterprise/